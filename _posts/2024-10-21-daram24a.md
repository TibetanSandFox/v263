---
title: Does Alignment help continual learning?
abstract: Backpropagation relies on instantaneous weight transport and global updates,
  thus questioning its neural plausibility. Continual learning mechanisms that are
  largely biologically inspired employ backpropagation as the baseline training rule.
  In this work, we examine the role of learning rules that avoid the weight transport
  problem in the context of continual learning. We investigate weight estimation approaches
  that use linear combinations of local and non-local regularization primitives for
  alignment-based learning. We couple these approaches with parameter regularization
  and replay mechanisms to demonstrate robust continual learning capabilities. We
  show that the layer-wise operations observed in alignment-based learning help to
  boost performance. We evaluated the proposed models in complex task-aware and task-free
  scenarios on multiple image classification datasets. We study the dynamics of representational
  similarity for learning rules compared to backpropagation. Lastly, we provide a
  mapping of the representational similarity to the knowledge preservation capabilities
  of the models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: daram24a
month: 0
tex_title: Does Alignment help continual learning?
firstpage: 48
lastpage: 55
page: 48-55
order: 48
cycles: false
bibtex_author: Daram, Anurag and Kudithipudi, Dhireesha
author:
- given: Anurag
  family: Daram
- given: Dhireesha
  family: Kudithipudi
date: 2024-10-21
address:
container-title: Proceedings of The Workshop on Classifier Learning from Difficult
  Data
volume: '263'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 10
  - 21
pdf: https://raw.githubusercontent.com/mlresearch/v263/main/assets/daram24a/daram24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
